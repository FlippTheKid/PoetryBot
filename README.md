# PoetryBot
Генеративная нейросеть обученная на стихах Пушкина и Лермонтова

Изначально мне было интересно, что будет если обучить какую-либо RNN модель генерировать стихи.
Изначально была выбрана следующая стратегия:
1) Отобрать корпус текстов для обучения
2) Токенизировать до слов, удалить знаки препинания
3) Предобучить word2vec на этих текстах
4) Подать на вход embedding слоя LSTM последовательности индексов слов, которые преобразуются в предобученные word2vec векторы
5) Далее эти векторы дообучаются в LSTM. Изначально, я пробовал предобработать так, чтобы в качестве train выборки выступала строка, а target - последнее слово строки и специальный токен EOL(end of line), однако успехом эта стратегия не увенчалась - генерация была крайне посредственной.
6) Исходя из полученных результатов, стало понятно, что нужно менять подход. Я взял куда более увесистый датасет с поэмами и стихами из интернета, и начал обучать на нем, при этом в качестве train выборки выступали слова (text[:-1]), а target - сдвитнутые на один индекс вперед слова (text[1:]). Используя новый, довольно большой датасет, возникли проблемы с нехваткой пасяти на ГПУ (cuda out of memory), исходя из чего пришлось сделать обрезку входных векторов до небольшого размера max_seq_len. Проблемы с памятью ушли, нейросеть обучилась, однако результат все еще довольно посредственный.
7) Далее я решил попробовать другой подход - Генерация посимвольно. В данном случае, словарь word_to_index стал гораздо меньшего размера - порядка 100 (было 250000 токенов в первом случае). Тут я взял свой датасет и обучил на нем. В отличие от предыдущего подхода, сочетания слов были лучше в плане сопоставления форм, однако смысловая нагрузка и семантика были лучше в первом подходе.

Вывод:
Первый подход - лучше в плане смысловой нагрузки и семантики
Второй подход - лучше в плане сопоставления форм слов и синтаксиса
